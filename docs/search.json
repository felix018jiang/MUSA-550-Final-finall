[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyzing Parking Trends in San Francisco, CA",
    "section": "",
    "text": "Samriddhi Khare, Roshini Ganesh Final Project, MUSA 550"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Analyzing Parking Trends in San Frncisco, CA",
    "section": "",
    "text": "San Francisco grapples with persistent challenges in its parking landscape, characterized by limited availability and high demand. The city’s densely populated neighborhoods often experience a shortage of parking spaces. These shortges are also reflective of certain social and demographic patterns in the city, as we have uncovered in this research.\nIn response to these difficulties, the use of parking apps has gained prominence, offering real-time information on available spaces and facilitating a more efficient navigation of the city’s intricate parking network. Increasing the efficiency of such apps could be a significant use case of this analysis.\nMoreover, the city has embraced innovative approaches to address parking issues. The implementation of smart parking meters, capable of adjusting rates based on demand and time of day, represents one such effort. Data from these parking meters has been included in our analysis. Additionally, there has been a push towards dynamic pricing strategies to incentivize turnover and optimize the utilization of parking spaces. These strategies can be more equitably informed if they take in to account how these policies are place within the larger sociodemographic framework of the city. As San Francisco continues to grapple with the complexities of urban parking, these trends underscore a broader shift towards sustainable transportation options and technological solutions in managing the city’s parking challenges."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Analyzing Parking Trends in San Frncisco, CA",
    "section": "Find out more",
    "text": "Find out more\nThis project aims to utilize San Francisco’s open parking data to map and visualize parking availability, occupancy, and clustering trends within the city over the recent months/years. We utilized data from numerous sources to make inferences about parking trends in the city. We included data from:\n\nOpen parking dataset with locations\nParking meter data to cross-validate areas of high parking activity by recorded transactions\nOn-Street Parking census, which provides counts of publicly available, on-street parking for each street segment\nCensus data (using the API) for the selected geography\nOSM Street Maps data for street network analysis\n\nThe code for this repository is hosted on our GitHub repository."
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Analyzing Parking Trends in San Francisco, California",
    "section": "",
    "text": "Samriddhi Khare, Roshini Ganesh\n\n\nFinal Project, MUSA 550\nThis project aims to utilize San Francisco’s open parking data to map and visualize parking availability, occupancy, and clustering trends within the city over the recent months/years. Data from numerous sources have been utilized to make inferences about parking trends in the city. These data repositories include:\n\nParking meter data to cross-validate areas of high parking activity by recorded transactions\nCensus Bureau data using the API for the selected geography\nOSM Street Maps data for street network analysis\n\n\n\nFile setup and data collection\nThe first step of this analysis comprises the essential tasks of loading necessary packages, configuring different APIs for data collection, and managing global environment settings.\n\n\nCode\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\nThis step involves gathering data on parking for 2023 and preliminary data cleaning for the large dataset. All geospatial datasets are set to a uniform coordinate reference system, and boundary shapefiles are primed for use in OSM street network API.\n\n\nCode\n#np.seterr(invalid=\"ignore\");\n\n#parking meter data\n\nmeters = pd.read_csv('.\\data\\Parking_Meters_20231220.csv')\n# Convert to geodataframe\n\ngeometry = [Point(xy) for xy in zip(meters['LONGITUDE'], meters['LATITUDE'])]\nmeters = gpd.GeoDataFrame(meters, geometry=geometry)\nmeters.crs = 'EPSG:4326'\n\nmeters = meters.to_crs('EPSG:3857')\n\n# neighborhoods in sf\n\n#neighborhoods = gpd.read_file(\"./data/Analysis_Neighborhoods.geojson\")\n\n#neighborhoods = neighborhoods.to_crs('EPSG:3857')\n\n#bay area counties and sf county geometries\n\n#bay_area_counties = gpd.read_file(\"./data/bayarea_county.geojson\")\n#bay_area_counties = bay_area_counties.to_crs('EPSG:4326')\n\n\n#sf_county =  bay_area_counties[bay_area_counties['COUNTY'] == 'San Francisco']\n\n#sf_poly = sf_county.iloc[0]\n#sf_poly = sf_poly.geometry\n\n#from shapely.ops import cascaded_union\n\n#sf_poly = cascaded_union(sf_poly)\n\n#print(type(sf_poly))\n\n#bay_area_counties.head()\n\n\n\nParking meters in San Francicso\nThe interactive map below visually represents the distribution of parking meters in San Francisco, showcasing distinct levels of aggregation. Notably, a concentrated area with high meter density emerges in the northeast region, coinciding with the presence of major tech company headquarters. However, drawing conclusive insights from the map alone is challenging. Considerations such as street network density become key determinants of parking availability. Therefore, it is essential to contextualize this data with factors like street networks and population variables. It is crucial to recognize that a high availability of parking does not necessarily indicate an absence of scarcity; demand may still surpass supply.\n\n\nCode\n# All coords\ncoords = meters[[\"LATITUDE\", \"LONGITUDE\"]] # Remember, (lat, lon) order\n\n# let's center the map on Philadelphia\nm = folium.Map(\n    location=[37.77, -122.43], zoom_start=12, tiles=xyzservices.providers.CartoDB.DarkMatter\n)\nfolium.plugins.FastMarkerCluster(data=coords).add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nOpen Street Map Data\nTo streamline the workflow with this large dataset, relevant OSM data is refined by excluding highways, where parking is not allowed. This ensures the dataset focuses solely on accessible areas with available parking spaces. A new graph is created and plotted to reflect only the non-highway streets.\n\n\nCode\ncity_name = 'San Francisco, California, USA'\nG = ox.graph.graph_from_place(city_name, network_type='drive', simplify=False, retain_all=True)\n#ox.plot_graph(G, bgcolor='k', node_color='w', node_size=5, edge_color='w', edge_linewidth=0.5)\n\n\n\n\n\nCode\n# Filter out highways (e.g., motorways, trunk roads)\nnon_highway_edges = [(u, v, key) for u, v, key, data in G.edges(keys=True, data=True) if 'highway' not in data or 'highway' in data and data['highway'] != 'motorway']\n\n# Create a new graph with non-highway streets\nG = G.edge_subgraph(non_highway_edges)\n\n# Plot the non-highway street network\n#ox.plot_graph(G, bgcolor='k', node_color='w', edge_color='w', node_size=5, edge_linewidth=0.5)\n\n\nWith this joined dataset as the base, the following transformations are performed:\n\nFirst, the latitude and logitude attributes for the cleaned OSM data are defined in xy coordinates to allow calculations related to location.\nNext, the nearest_edges function is used to determine the closest street edge to each parking meter.\nThird, the city’s parking meter data is integrated with the OSM Street Network data using the merge function to associate parking-related details with their corresponding street locations and the larger surrounding road infrastructure.\nThe joined dataset is then cleaned to:\n\nDrop columns that do not contribute to this study,\nEliminate streets that have zero parking meters, and\nRemove outlier values to only retain street lengths between 0 and 100 meters. This is done to ensure that the count of meters per street is normalized across the dataset. This is important because a 5-mile street segment could inherently accommodate more meters than a 1-mile street segment. The constraints distill the original dataset into a comparable dataset of street & parking factors.\n\n\n\n\nCode\nsf_edges = ox.graph_to_gdfs(G, edges=True, nodes=False)\n\nG_projected = ox.project_graph(G, to_crs= 'EPSG:3857' )\n\n# Define the longitude and latitude columns in meters\nx = meters['geometry'].x\ny = meters['geometry'].y\n\n# Use the nearest_edges function to find the nearest edge for each parking meter\nnearest_edges = ox.distance.nearest_edges(G_projected, X=meters.geometry.x, Y=meters.geometry.y)\n\nmeters_nodes = pd.DataFrame(nearest_edges, columns=['u', 'v', 'key'])\nmeters_nodes['Count'] = 1\n\n\ngrouped = meters_nodes.groupby(['u', 'v'])['Count'].sum().reset_index()\nmerged_gdf = sf_edges.merge(grouped, on=['u', 'v'], how='left')\nmerged_gdf = merged_gdf.loc[merged_gdf['Count'] &gt; 0]\n\n# List of columns to drop\ncolumns_to_drop = ['u', 'v', 'osmid', 'oneway', 'lanes', 'ref', 'maxspeed', 'reversed', 'access', 'bridge', 'junction', 'width', 'tunnel']\n\n# Drop the specified columns\nmerged_gdf = merged_gdf.drop(columns=columns_to_drop)\n\nmerged_gdf['truecount'] = merged_gdf['Count'] / merged_gdf['length']\n\n#removing outliers\n\n# Assuming merged_gdf is your DataFrame and 'length' is the name of the column\ncolumn_name = 'length'\n\n# Create a boolean mask to filter rows based on the condition\nmask = (merged_gdf[column_name] &gt;= 10) & (merged_gdf[column_name] &lt;= 100)\n\n# Apply the mask to the DataFrame\nmerged_gdf = merged_gdf[mask]\n\n#merged_gdf.head()\n\n\n\n\n\n\nParking Meter Distribution Analysis:Number of Parking Meters by length per Street\nOn analyzing the number of parking meters by street segment, disparities in the distribution of parking meters become apparent. Parking meters tend to be concentrated in downtown areas such as Union Square and Fisherman’s Wharf. An interesting contrast however is observed in Nob Hill in the downtown area which exhibits a low density of parking meters. This prompts an inquiry into the factors contributing to variations in meter density even within prominent city centers. In the subsequent sections of this study, demographic factors that affect parking distribution are explored.\nWhile outside of the purview of this analysis, questions also arise about other factors that influence the siting of parking meters including meter make, meter activity, and meter revenue, which may be important contributors to parking siting decisions. These are to be included in future iterations of this study.\n\n\nCode\nmerged_gdf.explore(tiles='cartodbdark_matter', column = 'truecount')\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Analyzing Parking Trends in San Francisco, California",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Analyzing Parking Trends in San Francisco, California",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "Samriddhi Khare (top) is currently a Research Fellow at Econsult Solutions’ Center for Future of Cities. In 2024, she received a Master’s in City Planning from the University of Pennsylvania, focusing on cities, data, and technology.\nRoshini Ganesh (bottom) is a City Planning candidate at UPenn interested in building connected, inclusive, and resilient smart cities using ethical technology. She currently works in transportation planning at Amtrak.\nTogether, we share an enthusiam for all things data and cities. This project was conceptualized during a professional visit we made to San Francisco in late 2023, where we observed first hand the city’s overt reliance on personal vehicles as the preferred mode of transportation and the equity and sustainabilty challenges that come with it.\nWe have created this project using Python, rendering on Quarto and published through Github pages. You can find more information about us on our personal websites linked above."
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nWe divided our analysis in to three distinct steps. Each sub-section highlights different types of analyses and visualizations:\n\nExploratory Analysis: Exploring the parking meter dataset to understand the relationships between the variables included.\nCensus API Data Collection: Collecting socio-economic variables from the American Community Survey fro the year 2021 and joining it to our street and meters data to explore patterns.\nSpatial Processes and Correlations: Exploring the relationships between variables after performing the join and observing correlations between calculated and chosed metrics."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Analyzing Parking Trends in San Francisco, CA",
    "section": "Introduction",
    "text": "Introduction\nSan Francisco grapples with persistent challenges in its parking landscape, characterized by limited availability and high demand. The city’s densely populated neighborhoods often experience a shortage of parking spaces. These shortges are also reflective of certain social and demographic patterns in the city, as we have uncovered in this research.\nIn response to these difficulties, the use of parking apps has gained prominence, offering real-time information on available spaces and facilitating a more efficient navigation of the city’s intricate parking network. Increasing the efficiency of such apps could be a significant use case of this analysis.\nMoreover, the city has embraced innovative approaches to address parking issues. The implementation of smart parking meters, capable of adjusting rates based on demand and time of day, represents one such effort. Data from these parking meters has been included in our analysis. Additionally, there has been a push towards dynamic pricing strategies to incentivize turnover and optimize the utilization of parking spaces. These strategies can be more equitably informed if they take in to account how these policies are place within the larger sociodemographic framework of the city. As San Francisco continues to grapple with the complexities of urban parking, these trends underscore a broader shift towards sustainable transportation options and technological solutions in managing the city’s parking challenges."
  },
  {
    "objectID": "index.html#data-sources",
    "href": "index.html#data-sources",
    "title": "Analyzing Parking Trends in San Francisco, CA",
    "section": "Data Sources",
    "text": "Data Sources\nThis project aims to utilize San Francisco’s open parking data to map and visualize parking availability, occupancy, and clustering trends within the city over the recent months/years. We utilized data from numerous sources to make inferences about parking trends in the city. We included data from:\n\nOpen parking dataset with locations\nParking meter data to cross-validate areas of high parking activity by recorded transactions\nOn-Street Parking census, which provides counts of publicly available, on-street parking for each street segment\nCensus data (using the API) for the selected geography\nOSM Street Maps data for street network analysis\n\nThe code for this repository is hosted on our GitHub repository."
  },
  {
    "objectID": "analysis/5-folium.html",
    "href": "analysis/5-folium.html",
    "title": "Spatial Processes and Correlations",
    "section": "",
    "text": "Spatial Join: Census OSM and Meter locations\nUpon hovering over areas of interest, the count of meters and associated variables, including wealth and demographic indicators, are observed. Preliminary findings suggest a correlation between areas with a high density of parking meters and indicators of affluence, such as higher socioeconomic status and a predominantly white population. Further analysis aims to uncover additional factors contributing to the observed patterns.\n\n\nCode\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# joining census and OSM data \n\n#columns_to_drop = ['in']\n\n#sf_final.drop(columns=columns_to_drop, inplace=True)\nsf_final = gpd.read_file(\"./data/census.geojson\")\nmerged_gdf = gpd.read_file(\"./data/merged_gdf.geojson\")\n\nsf_final = sf_final.to_crs('EPSG:3857')\n\nmerged_gdf = merged_gdf.to_crs('EPSG:3857')\n\nfinal_gdf = gpd.sjoin(merged_gdf, sf_final, how='left', op='intersects')\n\n#columns_to_drop = ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'INTPTLAT', 'INTPTLON']\n\n#final_gdf.drop(columns=columns_to_drop, inplace=True)\n\n\n#final_gdf.head()\n\n\nD:\\Fall_2023\\Python\\Mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\nCode\n#another map showing census data by street\n\nfinal_gdf.explore(column=\"Count\", tiles=\"cartodbdark_matter\")\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCorrelations\nTo understand distribution trends with more precision, a correlation test is conducted to identify the variables that exhibit the strongest statistical relationship with the count of parking meters. This information on influencing variables could inform predictive modeling for the future placement of parking meters.\n\n\nCode\n# correlations\n\n# The feature columns we want to use\nimport seaborn as sns\n\n\ncols = [\n    \"Count\",\n    \"truecount\",\n    \"length\",\n 'Total_Pop', \n    'Med_Inc', \n    'White_Pop', \n    'Travel_Time', \n    'Means_of_Transport', \n    'Total_Public_Trans', \n    'Med_Age', \n    'workforce_16', \n    'Num_Vehicles', \n    'households_NOcar_pct', \n    'households_car_pct', \n    'commute_30_90min_pct', \n    'Drive_Work_pct', \n    'PublicTransport_work_pct', \n    'Percent_White', \n    'Mean_Commute_Time', \n    'Percent_Taking_Public_Trans'\n]\n\n\n\n# Trim to these columns and remove NaNs\nfinal_corr = final_gdf[cols + [\"geometry\"]].dropna()\n#final_corr.head()\n\n\n\n\nCode\nplt.figure(figsize=(15, 15))\nsns.heatmap(final_corr[cols].corr(), cmap='coolwarm', annot=True, vmin=-1, vmax=1)\n\nplt.show()\n\n\n\n\n\nWe can see here that factors like percentage of people taking public transit and residents having longer commute times have a negative correlation with number of parking meters per street, indicating that an increase in these variables can be associated with meter-rich areas.\n\n\nConclusion and Next Steps\nThe patterns we have uncovered through this analysis not only sheds light on the current state of parking demand but also equips us with the predictive tools needed to anticipate future trends. Harnessing this knowledge can enable city officials and policymakers to proactively address the growing challenges of parking management, creating sustainable solutions that cater to the specific needs of different communities."
  },
  {
    "objectID": "analysis/4.5-folium.html",
    "href": "analysis/4.5-folium.html",
    "title": "Census API and Data Collection",
    "section": "",
    "text": "Code\n# Import packages\n\nimport altair as alt\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport pandas as pd\n#import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\nimport requests\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\nimport folium\nfrom folium import plugins\nfrom shapely.geometry import Point\nimport xyzservices\nimport osmnx as ox\nimport networkx as nx\nimport pygris\nimport cenpy\n\n\n\n%matplotlib inline\n\n# See lots of columns\npd.options.display.max_rows = 9999 \npd.options.display.max_colwidth = 200\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensus API Data\nIn this section, an API query is generated to retrieve demographic data for San Francisco through the American Community Survey (ACS) 5-year survey for the year 2021. The variables selected for analysis include the white population, Hispanic or Latino population, median income, and the population that commutes by driving. These variables are deemed significant for understanding socioeconomic and commuting patterns after examining multiple variables. This information is brought in at the tract level to capture localized nuances. It is then joined to the working dataset to examine parking trends in the context of demographic associations at a granular level.\n\n\nCode\n#available = cenpy.explorer.available()\n#available.head()\n\n# Return a dataframe of all datasets that start with \"ACS\"\n# Axis=0 means to filter the index labels!\n#acs = available.filter(regex=\"^ACS\", axis=0)\n\n# Return a dataframe of all datasets that start with \"ACSDT5Y\"\n#available.filter(regex=\"^ACSDT5Y\", axis=0)\n#acs = cenpy.remote.APIConnection(\"ACSDT5Y2019\")\n#acs.variables.head(n=100)\n\n\n#looking for variables\n\n#income_matches = acs.varslike(\n#    pattern=\"MEDIAN HOUSEHOLD INCOME IN THE PAST 12 MONTHS\",\n#    by=\"concept\",  # searches along concept column\n#).sort_index()\n\n#race_matches\n\n#race_matches = acs.varslike(\n#        pattern=\"WHITE\",\n#    by=\"concept\",  # searches along concept column\n#).sort_index()\n\n#race_matches\n\n#drive choice\n\n#drive_matches = acs.varslike(\n#        pattern=\"transportation\",\n #   by=\"concept\",  # searches along concept column\n#).sort_index()\n\n#drive_matches\n\n\n\n\nCode\n#variables = [\n#    \"NAME\",\n #   \"B19013_001E\", # med inc\n #   \"B03002_001E\", # Total\n #   \"B03002_003E\", # Not Hispanic, White\n #   \"B03002_004E\", # Not Hispanic, Black\n #   \"B03002_005E\", # Not Hispanic, American Indian\n #   \"B03002_006E\", # Not Hispanic, Asian\n #   \"B03002_007E\", # Not Hispanic, Native Hawaiian\n #   \"B03002_008E\", # Not Hispanic, Other\n #   \"B03002_009E\", #  Two or More Races\n #   \"B03002_012E\"]  # hisp\n\n#Med_Age = B01002_001E,\n#     White_Pop = B02001_002E,\n#     Travel_Time = B08013_001E,\n#     Num_Commuters = B08012_001E,\n#     Means_of_Transport = B08301_001E,\n#     Total_Public_Trans = B08301_010E,\n#     workforce_16 = B08007_001E,\n#     Num_Vehicles = B06012_002E,\n\n\n#counties = cenpy.explorer.fips_table(\"COUNTY\")\n#counties.head()\n\n# Search for rows where name contains \"San Francisco\"\n#counties.loc[ counties[3].str.contains(\"San Francisco\") ]\n\n#sf_county_code = \"075\"\n#ca_state_code = \"06\"\n\n#sf_inc_data = acs.query(\n#    cols=variables,\n#    geo_unit=\"block group:*\",\n#    geo_filter={\"state\": ca_state_code, \"county\": sf_county_code, \"tract\": \"*\"},\n#)\n\n\n#sf_inc_data.head(700)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt this point in our analysis, we were able to collect race and income variables from the census API, but were running into errors while trying to include other variables invloving drive time to work and preferred mode of transportation. To fix this error, we performed the census API call in R and joined that data to our existing dataset. The variables that we were unable to join are commented on the code chunk above. The R script used is available on the project repository.\n\n\n\n\nCode\n#convert to float \n\n#for variable in variables:\n#    # Convert all variables EXCEPT for NAME\n#    if variable != \"NAME\":\n#        sf_inc_data[variable] = sf_inc_data[variable].astype(float)\n        \n        \n\n\n\n\nCode\n#merges\n#sf_inc_data.rename(columns={'B19013_001E': 'Median Income',  \"B03002_001E\": \"Total\",  # Total\n#        \"B03002_003E\": \"White\",  # Not Hispanic, White\n#        \"B03002_004E\": \"Black\",  # Not Hispanic, Black\n#        \"B03002_005E\": \"AI/AN\",  # Not Hispanic, American Indian\n#        \"B03002_006E\": \"Asian\",  # Not Hispanic, Asian\n#        \"B03002_007E\": \"NH/PI\",  # Not Hispanic, Native Hawaiian\n#        \"B03002_008E\": \"Other_\",  # Not Hispanic, Other\n#        \"B03002_009E\": \"Two Plus\",  # Not Hispanic, Two or More Races\n#        \"B03002_012E\": \"Hispanic\"}, inplace=True)\n\n#sf_inc_data = sf_inc_data.loc[sf_inc_data['Median Income'] &gt; 0]\n\n# sf_block_groups = pygris.block_groups(\n#     state=ca_state_code, county=sf_county_code, year=2021\n# )\n# sf_final = sf_block_groups.merge(\n#     sf_inc_data,\n#     left_on=[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\"],\n#     right_on=[\"state\", \"county\", \"tract\", \"block group\"],\n# )\n\n# #writing the geojson to use in r\n\n# #bringing back the complete dataset\n\n# #sf_final.to_file(output_file, driver='GeoJSON')\n\nsf_final = gpd.read_file(\"./data/census.geojson\")\n\nsf_final = gpd.sjoin(sf_final, sf_block_groups, how=\"inner\", op=\"intersects\")\n\ncolumns_to_drop = ['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT', 'INTPTLON','index_right']\n\nsf_final.drop(columns=columns_to_drop, inplace=True)\n\n#sf_final.head()\n\n#print(type(sf_final))\n\n\nD:\\Fall_2023\\Python\\Mambaforge\\envs\\musa-550-fall-2023\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n  if await self.run_code(code, result, async_=asy):\n\n\n\n\nCode\n#columns_to_drop = ['index_right']\n\n#sf_final.drop(columns=columns_to_drop, inplace=True)\n\n#sf_final.head()\n\n#column_names = sf_final.columns.tolist()\n#print(column_names)\n\n\n\n\nExploratory analysis of census variables\nFirst, the median income map is examined to discern patterns, if any, between neighborhood wealth and parking meter density. It is hard to draw any meaningful conclusions from this map alone, as we need to join the street and parking meter data to see where the overlaps occur.\n\nData by Census Tract\n\n\nCode\n# plot\n\n#sf_final.explore(column=\"Med_Inc\", tiles=\"cartodbdark_matter\")\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  }
]